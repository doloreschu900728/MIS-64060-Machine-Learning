---
title: "Machine Learining_Project 1"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

Read in file.Intall caret.
```{r}
UBdata<-read.csv("UniversalBank.csv")
head(UBdata)
library(caret)
```
Transform categorical predictors with more than two categories into dummy variables. In this case, Education.
```{r}
UBdata$Education<-as.factor(UBdata$Education)
dummy_model<-dummyVars(~Education,data=UBdata)
head(predict(dummy_model, UBdata))
Education_dummy<-predict(dummy_model, UBdata)
UBdata<-cbind(UBdata, predict(dummy_model, UBdata))
UBdata$Education<- NULL
UBdata
```

Nomralize Data
```{r}
Norm_UBdata<-preProcess(UBdata[, c(2, 3, 4, 6, 7, 8)], method = c("center", "scale"))
UBdata_normalized<- predict(Norm_UBdata,UBdata)
summary(UBdata_normalized)
```

Partition the data into training (60%) and validation (40%) sets.
```{r}
set.seed(123)
Train_Index<-createDataPartition(UBdata_normalized$Personal.Loan, p = 0.6, list=FALSE)
Train_Data<-UBdata_normalized[Train_Index,]
Validation_Data<-UBdata_normalized[-Train_Index,]
```

Examine that that 2 sets have similar distribution in personal loan acceptance rate
```{r}
summary(Train_Data$Personal.Loan)
summary(Validation_Data$Personal.Loan)
```

Train the model using the training set.
```{r}
set.seed(123)
UBdata_normalized$Personal.Loan<- as.factor(UBdata_normalized$Personal.Loan)
Search_grid <- expand.grid(k = c(2, 3, 5, 7, 9))
model<-train(Personal.Loan~Age+Experience+Income+Family+CCAvg+Mortgage+Securities.Account+CD.Account+Online+CreditCard+Education.1+Education.2+Education.3, data = Train_Data, method = "knn", tuneGrid = Search_grid)
model
```

1. Use the model with k = 5 to predict the given case, since k = 5 leads to the highest accuracy. 
```{r}
library(class)
library(ISLR)
Test_Customer<- data.frame("Age" = 40, "Experience" = 10, "Income" = 84, "Family" = 2, "CCAvg" = 2, "Education_1" = 0, "Education_2" = 1, "Education_3" = 0, "Mortgage" = 0, "Securities Account" = 0, "CD Account" = 0, "Online" = 1, "Credit Card" = 1)

Train_Predictors <- Train_Data[, c(2:4, 6:8, 10:16)]
Customer_Predictors <- Test_Customer
Train_Labels <- Train_Data[, 9]
Predicted_customer_labels <- knn(Train_Predictors, Customer_Predictors, cl = Train_Labels, k = 5)
Predicted_customer_labels
#According to the model, this customer would be classified as successful (loan accepted).
```

2. What is a choice of k that balances between overfitting and ignoring the predictor information?
```{r}
library(gmodels)
Validation_Predictors <- Validation_Data[, c(2:4, 6:8, 10:16)]
Validation_Labels <- Validation_Data[, 9]
Predicted_Validation_Labels <- knn(Train_Predictors, Validation_Predictors, cl = Train_Labels, k = 5)
CrossTable(x = Validation_Labels, y = Predicted_Validation_Labels)
```
K = 5.



3. Show the confusion matrix for the validation data that results from using the best k.
```{r}
Validation_Predictors <- Validation_Data[, c(2:4, 6:8, 10:16)]
Validation_Labels <- Validation_Data[, 9]
Predicted_Validation_Labels <- knn(Train_Predictors, Validation_Predictors, cl = Train_Labels, k = 5)
CrossTable(x=Validation_Labels, y=Predicted_Validation_Labels, prop.chisq = FALSE)
```


4. Consider the following customer: Age = 40, Experience = 10, Income = 84,
Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0,
Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and Credit
Card = 1. Classify the customer using the best k.
```{r}
Customer_2_Predictors <- data.frame("Age" = 40, "Experience" = 10, "Income" = 84,
"Family" = 2, "CCAvg" = 2, "Education_1" = 0, "Education_2" = 1, "Education_3" = 0,
"Mortgage" = 0, "Securities Account" = 0, "CD Account" = 0, "Online" = 1., "Credit
Card" = 1)
Predicted_customer_2_label <- knn(Train_Predictors, Customer_2_Predictors, cl = Train_Labels, k = 5)
Predicted_customer_2_label
```
This customer is also classified as successful. 

5. Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason.
```{r}
set.seed(123)
Train_Index = createDataPartition(UBdata_normalized$Personal.Loan, p = 0.5, list = FALSE)
Train_Data = UBdata_normalized[Train_Index,]
Non_Train_Data = UBdata_normalized[-Train_Index,]

Search_grid <- expand.grid(k = c(3, 5, 7))
model_2<-train(Personal.Loan~Age+Experience+Income+Family+CCAvg+Mortgage+Securities.Account+CD.Account+Online+CreditCard+Education.1+Education.2+Education.3, data = UBdata_normalized, method = "knn", tuneGrid = Search_grid)
model_2



Validation_Index = createDataPartition(Non_Train_Data$Personal.Loan, p = 0.6, list = FALSE)
Validation_Data = Non_Train_Data[Validation_Index, ]
Test_Data = Non_Train_Data[-Validation_Index,]
summary(Train_Data$Personal.Loan)
summary(Validation_Data$Personal.Loan)
summary(Test_Data$Personal.Loan)
Train_Predictors <- Train_Data[, c(2:4, 6:8, 10:16)]
Validation_Predictors <- Validation_Data[, c(2:4, 6:8, 10:16)]
Test_Predictors <- Test_Data[, c(2:4, 6:8, 10:16)]
Train_Labels <- Train_Data[, 9]
Validation_Labels <- Validation_Data[, 9]
Test_Labels <- Test_Data[, 9]
Predicted_Train_Labels <- knn(Train_Predictors, Train_Predictors, cl = Train_Labels, k = 5)
Predicted_Validation_Labels <- knn(Train_Predictors, Validation_Predictors, cl = Train_Labels, k = 5)
Predicted_Test_Labels <- knn(Train_Predictors, Test_Predictors, cl = Train_Labels, k = 5)
head(Predicted_Test_Labels)
```

Comparing the confusion matrix of test set and the training and validation, the train set has higher true positive rate compared to the validation and the test set, meaning that the the model was better at detecting those who would accept personal loans in the test set. The model is able to detect true negatives, that is, accurately calling out those who would not accept the loan, for almost 100% of the time in all three sets. 
```{r}
CrossTable(x = Train_Labels, y = Predicted_Train_Labels)
CrossTable(x = Validation_Labels, y = Predicted_Validation_Labels)
CrossTable(x = Test_Labels, y = Predicted_Test_Labels)
```


