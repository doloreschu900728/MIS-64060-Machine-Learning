get.objective(lp_dual)
get.variables(lp_dual)
get.sensitivity.rhs(lp_dual)
install.packages("caret")
install.packages("caret")
class(b)
b<-1947.01
class(b)
cb_char<-haracter(b)
cb_char<-character(b)
b_char<-character(b)
b_char
class(b_char)
b_char
#3. Create a vector containing following mixed elements {1, 'a', 2, 'b'} and find out its class
vector1<-c(1, "a", 2, "b")
class(vector1)
vector1
seq(1,100,1)
list1<-list(1, "a")
class(list1)
?data.frame
#3. Create a dataframe and name it DF. This dataframe should contain Three columns with the following names C1, C2 and C3. C1 should contain numeric values 1 and 4.37. C2 should contain “Red” and “Blue” and C3 should contain TRUE and FALSE.
DF<-data.frame(C1 = c(1, 4.37), C2 = c("Red", "Blue"), C3 = c(TRUE,FALSE))
DF
#4. Select and print C1 column of the DF dataframe in the previous example.
DF(,1)
#4. Select and print C1 column of the DF dataframe in the previous example.
DF[,1]
# 5. Consider the following dataframe:
#DF<-data.frame(V1=1:6, Countries=c('US', 'UK','UK', 'India','China','India'))
#Show the frequency (i.e. count) of each of the countries in the data frame.
DF<-data.frame(V1=1:6, Countries=c('US', 'UK','UK', 'India','China','India'))
DF
?frequency(
)
?frequency()
summary(DF)
?summary
summary(DF)
# 6. Define a variable x=0.75. write a code to crat a variable y whose value is dependent on the value of x. If x is positive, y should be set to 14 otherwise it should be set -19.7. Change the value of x to -1 and evaluate your code again.
x<-0.75
if(x>0) {y<-14} else {y<--19.7}
print(x)
peint(y)
print(y)
if(x>0) {y<-14} else if (x=0) {y<-0) else {y<--19.7}
print(x)
print(y)
print(x)
print(y)
install.packages("Benchmarking")
university_data <- read.csv("Universities.csv")
library(caret)
library(caret)
library(tidyverse)
library(dplyr)
university_data <- na.omit(university_data)
university_data_cont <- university_data [, 4:20]
norm_model <- preProcess(university_data_cont, method = "range")
udata_cont_norm <- predict(norm_model, university_data_cont)
library(factoextra)
fviz_nbclust(udata_cont_norm, kmeans, method = "wss")
fviz_nbclust(udata_cont_norm, kmeans, method = "silhouette")
set.seed(123)
k2 <- kmeans(udata_cont_norm, centers = 2, nstart = 25)
k2$centers
university_data$cluster_label <- k2$cluster
library(ggplot2)
ggplot(university_data[university_data$cluster_label==1, ], aes(x = State, y = Public..1...Private..2.)) + geom_point()
ggplot(university_data[university_data$cluster_label==2, ], aes(x = State, y = Public..1...Private..2.)) + geom_point()
university_data$cluster_label <- k2$cluster
library(ggplot2)
ggplot(university_data[university_data$cluster_label==1, ], aes(x = State, y = Public..1...Private..2.)) + geom_point()
ggplot(university_data[university_data$cluster_label==2, ], aes(x = State, y = Public..1...Private..2.)) + geom_point()
Cluster_1 <- university_data[university_data$cluster_label == 1, ]
Cluster_1$College.Name
Cluster_2 <- university_data[university_data$cluster_label == 2, ]
Cluster_2$College.Name
?na.aggregate
university_data[university_data$College.Name == "Tufts University", ]
?get_dist
university_data <- read.csv("Universities.csv")
library(flexclust)
university_data[university_data$College.Name == "Tufts University", ]
university_data <- read.csv("Universities.csv")
university_data[university_data$College.Name == "Tufts University", ]
centroids <- k2$centers
cantroids
centroids
DF_1 <- rbind(university_data[university_data$College.Name == "Tufts University", ], centroids[1,])
centroids[1,]
university_data <-scale(university_data)
university_data_cont <- university_data[, 4:20]
university_data_cont_n <- scale(university_data_cont)
university_data[university_data$College.Name == "Tufts University", ]
university_data <- read.csv("Universities.csv")
university_data_[university_data$College.Name == "Tufts University", ]
university_data[university_data$College.Name == "Tufts University", ]
university_data_cont <- university_data[, 4:20]
university_data_cont_n <- scale(university_data_cont)
Tufts <- university_data_cont_n(476)
university_data <- read.csv("Universities.csv")
university_data[university_data$College.Name == "Tufts University", ]
university_data_cont <- university_data[, 4:20]
university_data_cont_n <- scale(university_data_cont)
Tufts <- university_data_cont_n(476)
Tufts <- university_data_cont_n[476]
Tufts
Tufts <- university_data_cont_n[476,]
Tufts
DF_1 <- rbind(Tufts, centroids[1,])
DF_1
DF_2 <- rbind(Tufts, centroids[2,])
distance_1 <- get_dist(DF_1)
distance_2 <- get_dist(DF_2)
distance_1 > distance_2
mean(university_data$X..PT.undergrad)
mean(na.omit(university_data)$X..PT.undergrad)
Tufts$X..PT.undergrad <- mean(na.omit(university_data)$X..PT.undergrad)
Tufts <- university_data_cont_n[476,]
Tufts$X..PT.undergrad <- mean(na.omit(university_data)$X..PT.undergrad)
Tufts
Tufts$X..PT.undergrad <-centroids$X..PT.undergrad[1,]
centroids$X..PT.undergrad
centroids$X..PT.undergrad
centroids
centroids <- k2$centers
colnames(cantroids)
colnames(centroids)
Tufts$X..PT.undergrad <-centroids[2, 7]
7]
Tufts
install.packages("dbscan")
library(Benchmarking)
x <- matrix(c(150, 400, 320, 520, 350, 320, 0.2, 0.7, 1.2, 2.0, 1.2, 0.7), ncol = 2)
y <- matrix(c(14000, 14000, 42000, 28000, 19000, 14000, 3500, 21000, 10500, 42000, 25000, 15000), ncol = 2)
colnames(x) <- c("Staff Hours per day", "Supplies per Day")
colnames(y) <- c("Reimbursed Patient-Days", "Privately Paid Patient Days")
FDH <- dea(x, y, RTS = "fdh")
CRS <- dea(x, y, RTS = "crs")
VRS <- dea(x, y, RTS = "vrs")
IRS <- dea(x, y, RTS = "irs")
DRS <- dea(x, y, RTS = "drs")
#FRH <- dea(x, y, RTS = "frh")
peers(FDH)
lambda(FDH)
peers(CRS)
lambda(CRS)
peers(VRS)
lambda(VRS)
peers(IRS)
lambda(IRS)
peers(DRS)
lambda(DRS)
#peers(FRH)
#lambda(FRH)
library(lpSolveAPI)
HV <- read.lp("HopeValley.lp")
HV
solve(HV)
get.objective(HV)
HV
get.variables(HV)
get.objective(HV)
get.variables(HV)
set.seed(2017)
X=runif(100)*10
Y=X*4+3.45
Y=rnorm(100)*0.29*Y+Y
plot(X~Y)
?lm()
cbind(X,Y)
data <- cbind(X,Y)
Model = lm(X~Y, data = data)
data <- cbind(X,Y)
Model = lm(X~Y, data = data)
data <- data.frame(X, Y)
Model = lm(X~Y, data = data)
summary(Model)
plot(Y~X)
Model = lm(Y~X, data = data)
summary(Model)
head(mtcars)
Model_1 = lm(wt~hp, data = mtcars)
Model_wt = lm(wt~hp, data = mtcars)
Model_mpg = lm(mpg~hp, data = metcars)
Model_mpg = lm(mpg~hp, data = mtcars)
summary(Model_wt)
summary(Model_mpg)
summary(Model_wt)
summary(Model_wt)
summary(Model_mpg)
Model_wt = lm(hp~wt, data = mtcars)
Model_mpg = lm(hp~mpg, data = mtcars)
summary(Model_wt)
summary(Model_mpg)
Model_mpg = lm(hp~mpg, data = mtcars)
?lm()
Model_cyl_mpg = lm(hp~cyl+mpg, data = mtcars)
summary(model)
summary(Model_cyl_mpg)
hp = 23.979 cly - 2.775 mpg + 54.067
hp = 23.979*cly - 2.775*mpg + 54.067
cly <- 4
mpg <- 22
hp = 23.979*cly - 2.775*mpg + 54.067
hp
predict(Model_cyl_mpg, data.frame(mpg = c(22), cly = c(4)))
predict(Model_cyl_mpg, data.frame(mpg = c(22), cyl = c(4)))
predict(Model_cyl_mpg, data.frame(mpg = c(22), cyl = c(4)), interval = "prediction", level = 0.85)
summary(Model_mpg)
install.packages('mlbench')
install.packages('mlbench')
library(mlbench)
data(BostonHousing)
head(BostonHousing)
Model <- lm(medv~crim+zn+ptratio+chas, data = BostonHousing)
summary(Model)
summary(Model_wt)
Price_Difference = -1.49367 * (15-18)
print(Price_Difference)
str(BostonHousing)
Model <- lm(medv~crim+zn+ptratio+chas, data = BostonHousing)
Model
summary(Model)
anova(Model)
Model
Model$residuals
set.seed(2017)
X=runif(100)*10
Y=X*4+3.45
Y=rnorm(100)*0.29*Y+Y
plot(Y~X)
data <- data.frame(X, Y)
Model = lm(Y~X, data = data)
summary(Model)
View(Model_1)
plot(data$X, Model$residuals, xlab = x, ylab = "Residuals")
hist(Model$residuals)
plot(data$X, Model$residuals, xlab = x, ylab = "Residuals")
length(data)
nrow(data)
X_SQ <- data$X^2
Model_2
Model_2 = Model = lm(Y~X_SQ, data = data)
summary(Model_2)
Model = lm(Y~X, data = data)
summary(Model)
X_SQ <- data$X^(1/2)
Model = lm(Y~X_SQ, data = data)
summary(Model)
plot(data$X, Model$residuals, xlab = "X", ylab = "Residuals")
getwd()
setwd("/Users/Dolores/Desktop/R/MIS-64060-Machine-Learning/Cereal")
setwd("/Users/Dolores/Desktop/R")
setwd("/Users/Dolores/Desktop/R/MIS-64060-Machine-Learning/Cereals")
data <- read.csv("Careals.csv")
data <- read.csv("Cereals.csv")
data <- na.omit(data)
is.na(data)
mean(is.na(data))
sum(is.na(data))
mydata <- data.frame(x = c(1, 2, 3), y = c(4, NA, 5), y = c(0,0,0))
mydata
mydata <- data.frame(x = c(1, 2, 3), y = c(4, NA, 5), y = c(0,0,NA))
mydata
mydata <- data.frame(x = c(1, 2, 3), y = c(4, NA, 5), z = c(0,0,NA))
na.omit(mydata, col = y)
na.omit(mydata, cols = "y")
mydata <- data.table(mydata)
?data.table
install.packages("data.table")
library(data.table)
mydata <- data.table(mydata)
na.omit(mydata, cols = "y")
data_norm <- scale(data)
?scale
str(data)
data_norm <- scale(data[, c(1:3)])
data[, c(1:3)
]
data_norm <- scale(data[, -c(1:3)])
data_norm
head(data_norm)
colnames(data)
install.packages("cluster")
data_norm <- cbind(data[, 1:3], scale(data[, -c(1:3)]))
head(data_norm)
distance <- dist(data_norm, method = "euclidean")
distance
distance <- dist(data_norm[, -c(1:3)], method = "euclidean")
distance
hc1 <- hclust(distance, method = "complete")
hc1
plot(hc1)
plot(hc1, cex = 0.6, hang = -1)
library(cluster)
hc_single <- agnes(data_norm, method = "single")
?agnes
hc_complete <- agnes(data_norm, method = "complete")
hc_average <- agens (data_norm, method = "average")
hc_average <- agnes (data_norm, method = "average")
hc_average
agnes?
agnes
?agnes
hc_single$ac
hc_complete$ac
hc_average$ac
?agnes
hc_ward <- agnes(data_norm, method = "ward")
hc_ward$ac
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes (Ward)")
summary(hc_ward)
---
title: "Customer Rating of Breakfast Cereals"
author: "Dolores Chu"
date: "11/13/2019"
output: html_document
---
##### The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for 77 breakfast cereals.
##### Data Preprocessing. Remove all cereals with missing values.
```{r}
data <- read.csv("Cereals.csv")
data <- na.omit(data)
```
##### a. Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward. Choose the best method.
First, normalize data
```{r}
data_norm <- cbind(data[, 1:3], scale(data[, -c(1:3)]))
```
Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements.
```{r}
distance <- dist(data_norm[, -c(1:3)], method = "euclidean")
hc1 <- hclust(distance, method = "complete")
plot(hc1, cex = 0.6, hang = -1)
```
hc_single <- agnes(data_norm, method = "single")
hc_complete <- agnes(data_norm, method = "complete")
hc_average <- agnes(data_norm, method = "average")
library(cluster)
hc_single <- agnes(data_norm, method = "single")
hc_complete <- agnes(data_norm, method = "complete")
hc_average <- agnes(data_norm, method = "average")
hc_ward <- agnes(data_norm, method = "ward")
hc_single$ac
hc_complete$ac
hc_average$ac
hc_ward$ac
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes (Ward)")
screeplot(hc_ward)
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes (Ward)")
hc_ward <- agnes(data_norm, method = "ward")
hc_single$ac
hc_complete$ac
hc_average$ac
hc_ward
?agnes()
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes (Ward)")
rect.hclust(hc_ward, k = 3)
distance <- dist(data_norm[, -c(1:3)], method = "euclidean")
hc_ward <- hclust(distance, method = "ward")
plot(hc1, cex = 0.6, hang = -1)
hc_ward <- hclust(distance, method = "ward.D")
plot(hc1, cex = 0.6, hang = -1)
hc_ward <- hclust(distance, method = "ward.D")
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 4, border = 1:4)
plot(hc1, cex = 0.6)
rect.hclust(hc_ward, k = 4, border = 1:4)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
rect.hclust(hclust(distance, method = "ward.D"), h = 9)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hclust(distance, method = "ward.D"), h = 9)
hc_ward <- hclust(distance, method = "ward.D")
rect.hclust(hclust(distance, method = "ward.D"), h = 9)
hc_ward <- agnes(data_norm, method = "ward")
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3, border = 1:3)
plot(hc1, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3)
plot(hc_ward, cex = 0.6, hang = -1)
hc_ward <- hclust(distance, method = "ward.D")
plot(hc_ward, cex = 0.6, hang = -1)
hc_ward <- agnes(data_norm, method = "ward")
plot(hc_ward, cex = 0.6, hang = -1)
hc_ward <- hclust(distance, method = "ward.D")
plot(hc_ward, cex = 0.6, hang = -1)
plot(hc_ward, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 3)
plot(hc_ward, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 6, border = 1:6)       # My dendrogram looks kinda weird... why?
plot(hc_ward, cex = 0.6, hang = -1)
rect.hclust(hc_ward, k = 6, border = 1:6)
library(caret)
ClusterA_index <- createDataPartition(p=0.95)
createDataPartition()
?plot(hc_ward, cex = 0.6, hang = -1)
?createDataPartition()
ClusterA_index <- createDataPartition(data_norm, p=0.95)
nrow(data_norm)
str(data_norm)
ClusterA_index <- createDataPartition(data_norm, p=0.95, list = FALSE)
install.packages("caTools")
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- as.numeric(rownames(ClusterA))
length(ClusterA_index)
nrow(data_norm)
ClusterB <- data_norm[-ClusterA_index,]
nrow(ClusterA)
nrow(ClusterB)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- as.numeric(rownames(ClusterA))
length(ClusterA_index)
nrow(data_norm)
ClusterB <- data_norm[-ClusterA_index,]
nrow(ClusterA)
nrow(ClusterB)
rownames(data_norm)
data_norm$index <- row.names(data_norm)
head(data_norm)
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- as.numeric(ClusterA$index)
length(ClusterA_index)
ClusterB <- data_norm[-ClusterA_index,]
nrow(ClusterA)
nrow(ClusterB)
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- as.numeric(ClusterA$index)
length(ClusterA_index)
ClusterA_index <- ClusterA$index
length(ClusterA_index)
ClusterB <- data_norm[-ClusterA_index,]
ClusterA_index <- as.list(ClusterA$index)
length(ClusterA_index)
ClusterB <- data_norm[-ClusterA_index,]
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- as.list(ClusterA$index)
length(ClusterA_index)
ClusterB <- data_norm[-ClusterA_index,]
class(ClusterA_index)
ClusterB <- data_norm[-c(ClusterA_index),]
c(ClusterA_index)
ClusterA$index
class(ClusterA_index)
as.list(as.numeric(ClusterA_index))
c(as.list(as.numeric(ClusterA_index)))
x<-data_norm$index
x
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
as.numeric(ClusterB$index)
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
as.numeric(ClusterA$index)
length(ClusterA_index)
y<-as.numeric(ClusterA$index)
x[y]
x
x<-as.numeric()
x<-as.numeric(x)
x
ClusterA$index
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
?split()
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA$index
ClusterB <- data_norm[-ClusterA,]
?df[]
?[]
ClusterA_index <- ClusterA$index
ClusterB <- data_norm[-ClusterA,]
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- ClusterA$index
ClusterB <- data_norm[-ClusterA,]
ClusterB <- data_norm[-ClusterA-index,]
data_norm$index <- row.names(data_norm)
set.seed(123)
ClusterA <- data_norm[sample(nrow(data_norm),67), ]
ClusterA_index <- ClusterA$index
ClusterB <- data_norm[-ClusterA-index,]
ClusterB <- data_norm[-ClusterA_index,]
